{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbXkUQWFLBRF"
   },
   "source": [
    "# HW2P2: Image Recognition and Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMg74_LaLL55"
   },
   "source": [
    "> This is the second homework assignment for **11-785: Introduction to Deep Learning**, which focuses on the task of **image verification**. In this assignment, we first train a custom **convolutional neural network (CNN)** to perform **face classification** over **8,631 distinct identities**.\n",
    ">\n",
    "> Once the model is trained, we use it to extract **face embeddings** for images. These embeddings are then compared across pairs of images to determine whether the two faces belong to the **same identity** or **different identities**, thereby solving the image verification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zcx2lHRylKP"
   },
   "source": [
    "## üìÖ Submission Schedule\n",
    "- ‚úÖ **Checkpoint:** Feburary 20, 2026 @ 11:59 PM (EST)  \n",
    "- üèÅ **Final Submission:** Feburary 27, 2026 @ 11:59 PM (EST)  \n",
    "  *Slack deadline: March 13, 2026 @ 11:59 PM (EST)*  \n",
    "- üíª **Code Submission:** \n",
    "  - Due: March 1st, 2026 @ 11:59PM (EST)\n",
    "  - Closes: March 13, 2026 @ 11:59 PM (EST) *or day-of via Slack*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcf4nWStyj1g"
   },
   "source": [
    "## Requirement Acknowledgement\n",
    "\n",
    "Setting the flag below to **True** indicates that you have read, understood, and agree to comply with **all** of the following requirements:\n",
    "\n",
    "1. **Slack Days Usage**  \n",
    "   Slack days may be used **only** for the **P2 Final submission**, *not* for the checkpoint.  \n",
    "   Specifically, slack days may be applied to submit **final P2 Kaggle scores** to the **Slack Kaggle Competition**, at the expense of your available slack days.\n",
    "\n",
    "2. **Final Code Submission Deadline**  \n",
    "   The final **Autolab code submission** is due **within 48 hours after** the Kaggle competition deadline,  \n",
    "   or **on the same day** as your final Kaggle submission‚Äîwhichever occurs first.\n",
    "\n",
    "3. **Kaggle Username & Score Verification**  \n",
    "   You must provide your **Kaggle username** below. We will use it to retrieve your official **PRIVATE leaderboard** score.  \n",
    "   Minor score or output variance is acceptable; however, any discrepancies caused by **modifications to the required submission code**\n",
    "   (including the notebook‚Äôs final submission cell) will result in an **Academic Integrity Violation (AIV)**.\n",
    "\n",
    "4. **Model Implementation Restrictions**  \n",
    "   You are **not permitted** to use pre-trained or pre-loaded models (e.g., from Hugging Face or similar libraries).  \n",
    "   You **may** implement models described in research papers or articles, but they **must** be implemented **from scratch**\n",
    "   using fundamental PyTorch components (e.g., `Linear`, `Conv2d`, etc.).\n",
    "\n",
    "5. **Data Usage Restrictions**  \n",
    "   The use of **any external data or datasets** is strictly prohibited at **any stage** of this assignment.\n",
    "\n",
    "6. **Collaboration Policy**  \n",
    "   You may collaborate with teammates to run experiments or ablations.  \n",
    "   However, you must submit **your own code** and **your own results**.\n",
    "\n",
    "7. **Academic Integrity**  \n",
    "   Failure to comply with any of the above requirements will be treated as an **Academic Integrity Violation (AIV)**.\n",
    "\n",
    "8. **Late Submissions**  \n",
    "   Late submissions **must** be submitted through the **Slack Kaggle Competition** (see write-up for details).  \n",
    "   Submissions made to the **regular Kaggle competition** after the original deadline will **not** be considered,\n",
    "   regardless of remaining slack days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2025-12-30T11:20:02.461513Z",
     "iopub.status.busy": "2025-12-30T11:20:02.460706Z",
     "iopub.status.idle": "2025-12-30T11:20:02.464938Z",
     "shell.execute_reply": "2025-12-30T11:20:02.464297Z",
     "shell.execute_reply.started": "2025-12-30T11:20:02.461483Z"
    },
    "id": "qrcE9PQnVAU_",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# @title Click to acknowledge then run cell!\n",
    "ACKNOWLEDGED = False # @param {\"type\":\"boolean\",\"placeholder\":\"False\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyCRnMfvp83m"
   },
   "source": [
    "## Welcome to the World of Convolutions!\n",
    "\n",
    "In the previous homework, you explored **Multi-Layer Perceptrons (MLPs)**‚Äîthe foundational building blocks of deep learning. You saw how MLPs can learn patterns from data, predict phonemes from speech, and even approximate complex functions. However, despite their expressive power, MLPs are not well-suited for image data. Flattening an image into a one-dimensional vector discards crucial **spatial information**, such as how neighboring pixels relate to one another.\n",
    "\n",
    "This limitation motivates the use of **Convolutional Neural Networks (CNNs)**. CNNs preserve spatial structure by operating directly on image grids, allowing models to learn meaningful visual patterns. In doing so, they bring us closer to designing systems that can truly *see* and interpret the visual world.\n",
    "\n",
    "In this homework, you will dive into CNNs through **face classification** and **face verification** tasks. In face classification, the goal is to identify a face as belonging to one of several known identities. In contrast, face verification asks a different question: *Do these two images belong to the same person?*‚Äîeven if that person was never seen during training.\n",
    "\n",
    "To make this concrete, imagine being given a photo of a friend and a group photo. A classifier would attempt to name your friend, while a verification model would simply determine whether the two images depict the same individual. CNNs enable this by learning **hierarchical features**: early layers capture edges and textures, while deeper layers encode higher-level concepts such as facial structures and distinctive features.\n",
    "\n",
    "A key theme of this assignment is understanding how **loss functions** shape model behavior. Classification models typically rely on **Cross-Entropy Loss**, which encourages correct label prediction. Verification tasks, however, depend on learning meaningful **embeddings**, where similarity in feature space reflects identity similarity. Loss functions such as **Triplet Loss** or **ArcFace Loss** are designed for this purpose, pulling embeddings of the same person closer together while pushing different identities apart. A well-trained verification model can even distinguish between visually similar individuals‚Äîsuch as siblings‚Äîwhile remaining robust to changes in pose, lighting, or expression.\n",
    "\n",
    "Beyond theory, this homework emphasizes **practical deep learning skills**. You will preprocess image data, apply data augmentation techniques (e.g., random cropping or flipping), and experiment with modern CNN architectures such as **ResNet** or **ConvNeXt**. Along the way, you will discover that a model optimized for classification is not automatically suitable for verification‚Äîhighlighting the importance of aligning architectural choices and loss functions with the task at hand.\n",
    "\n",
    "By the end of this assignment, you will have built a robust face recognition system inspired by real-world applications, from smartphone unlocking to identity verification systems. Let‚Äôs get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9EpnOYJ3kG1"
   },
   "source": [
    "# **SET-UP**\n",
    "\n",
    "\n",
    "Click [**HERE**](https://www.kaggle.com/t/12ca057eb27d4c62ae093a0f407e0651) to join the competition first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5VKNQO43kG2"
   },
   "source": [
    "## **Colab Users**\n",
    "Follow the steps below to set up your Google Colab environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgN5GgiS3kG2"
   },
   "source": [
    "### Step 1: Verify GPU\n",
    "Check that a GPU is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jS2BUwL03kG2"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgMiofF6OY7f"
   },
   "source": [
    "### Step 2: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0rbYdRn63kG4"
   },
   "outputs": [],
   "source": [
    "!pip install wandb==0.23.1 pytorch_metric_learning torchinfo kaggle==1.8.0 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQcaH4wBQO-j"
   },
   "source": [
    "### Step 3: Configure Kaggle API Access\n",
    "\n",
    "‚ö†Ô∏è **Important:** You must provide your own Kaggle API credentials for data downloads, while creating your deliverables for code submission and (optionally) submitting to the competition.\n",
    "\n",
    "#### How to obtain your Kaggle API credentials\n",
    "\n",
    "1. Navigate to your **Kaggle Profile ‚Üí Settings** tab.\n",
    "2. Click **‚ÄúGenetate New Token‚Äù**\n",
    "3. Copy and paste the access token below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2yGTFyBZL2g",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "os.environ['KAGGLE_USERNAME'] = \"<your-username>\" # TODO: Verify in Settings\n",
    "os.environ['KAGGLE_API_TOKEN'] = \"<your-key>\" # TODO: Add Access Token (must be the new token starting with \"KGAT_\")\n",
    "\n",
    "# Verify\n",
    "import kaggle\n",
    "api = kaggle.api  # Already authenticated on import\n",
    "api.competitions_list_cli()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4v1jGLi3kG5"
   },
   "source": [
    "### Step 4: Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGBDqD4w3kG5"
   },
   "outputs": [],
   "source": [
    "api.competition_download_files('11785-hw-2-p-2-face-verification-spring-2026')\n",
    "!unzip -qo /content/11785-hw-2-p-2-face-verification-spring-2026.zip -d /content/dataset\n",
    "!rm -rf /content/11785-hw-2-p-2-face-verification-spring-2026.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNM2T4QXhQvv"
   },
   "outputs": [],
   "source": [
    "!du -h --max-depth=2 /content/dataset/hw2p2_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8G-S6Qu3kG6"
   },
   "source": [
    "## **Kaggle Users**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ah--YMLE3kG6"
   },
   "source": [
    "### Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T11:20:13.784106Z",
     "iopub.status.busy": "2025-12-30T11:20:13.783505Z",
     "iopub.status.idle": "2025-12-30T11:20:18.614949Z",
     "shell.execute_reply": "2025-12-30T11:20:18.614292Z",
     "shell.execute_reply.started": "2025-12-30T11:20:13.784075Z"
    },
    "id": "MFJt80k03kG7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install wandb==0.23.1 pytorch_metric_learning torchinfo kaggle==1.8.0 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1yqsDZK3kG7"
   },
   "source": [
    "### Step 2: Get Data\n",
    "\n",
    "If you are using Kaggle, follow these steps to add the dataset directly to your notebook:\n",
    "1. Join the kaggle competition (https://www.kaggle.com/t/12ca057eb27d4c62ae093a0f407e0651)\n",
    "2. Open your **Kaggle Notebook**.  \n",
    "3. Navigate to **Notebook ‚Üí Input**.  \n",
    "4. Click **Add Input**.  \n",
    "5. Choose \"Competition Datasets\" and \"Your work\", and then you'll be able to see the competiation \"11785-hw-2-p-2-face-verification-spring-2026\"\n",
    "6. Click the **‚ûï (plus sign)** to add the dataset to your notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAlgC4eddPqY"
   },
   "source": [
    "### Step 3: Configure Kaggle API Access\n",
    "\n",
    "‚ö†Ô∏è **Important:** You must provide your own Kaggle API credentials while creating your deliverables for code submission and (optionally) submitting to the competition.\n",
    "\n",
    "#### How to obtain your Kaggle API credentials\n",
    "\n",
    "1. Navigate to your **Kaggle Profile ‚Üí Settings** tab.\n",
    "2. Click **‚ÄúGenetate New Token‚Äù**\n",
    "3. Copy and paste the access token below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOTrR8FLdPqb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "os.environ['KAGGLE_USERNAME'] = \"<your-username>\" # TODO: Verify in Settings\n",
    "os.environ['KAGGLE_API_TOKEN'] = \"<your-key>\" # TODO: Add Access Token (must be the new token starting with \"KGAT_\")\n",
    "\n",
    "# Verify\n",
    "import kaggle\n",
    "api = kaggle.api  # Already authenticated on import\n",
    "api.competitions_list_cli()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aD8oEOiY3kG8"
   },
   "source": [
    "## **PSC Users**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2M0BjQSTAOJ"
   },
   "source": [
    "### Step 1: Setting Up Your Environment on Bridges2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rsWFyRVTAOJ"
   },
   "source": [
    "‚ùóÔ∏è‚ö†Ô∏è For this homework, we are **providing a shared Conda environment** for the entire class. Therefore, PSC users **do not need to manually install any packages**.\n",
    "\n",
    "‚ùóÔ∏è‚ö†Ô∏è For this homework, you need to **download the dataset to the node `$LOCAL`** to avoid I/O bottlenecks from the shared filesystem. This means that each time you run on a new node, you need to download the dataset again. However, as long as you stay on the same node, you do not need to re-download the dataset. Please refer to **Step 3** for the detailed procedure.\n",
    "\n",
    "Follow these steps to set up the environment and start a Jupyter notebook on Bridges2:\n",
    "\n",
    "- To run your notebook more efficiently on PSC, we need to use a **Jupyter Server** hosted on a compute node.\n",
    "\n",
    "- You can use your prefered way of connecting to the Jupyter Server.  **The recommended way is to connect in VSCode.**\n",
    "Follow the instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpAJ7F-8TAOJ"
   },
   "source": [
    "\n",
    "\n",
    "#### **Connect in VSCode**\n",
    "SSH into Bridges2 and navigate to your **Jet directory** (`Jet/home/<your_psc_username>`). Upload your notebook there, and then connect to the Jupyter Server from that directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTb_H63kTAOJ"
   },
   "source": [
    "#### **1.1 SSH into Bridges2**\n",
    "1ÔºâOpen VS Code and click on the `Extensions` icon in the left sidebar. Make sure the \"**Remote - SSH**\" extension is installed.\n",
    "\n",
    "2ÔºâOpen the command palette (**Shift+Command+P** on Mac, **Ctrl+Shift+P** on Windows). A search box will appear at the top center. Choose `\"Remote-SSH: Add New SSH Host\"`, then enter:\n",
    "\n",
    "```bash\n",
    "ssh <your_username>@bridges2.psc.edu #change <your_username> to your username\n",
    "```\n",
    "\n",
    "Next, choose `\"/Users/<your_username>/.ssh/config\"` as the config file. A dialog will appear in the bottom right saying \"Host Added\". Click `\"Connect\"`, and then enter your password.\n",
    "\n",
    "(Note: After adding the host once, you can later use `\"Remote-SSH: Connect to Host\"` and select \"bridges2.psc.edu\" from the list.)\n",
    "\n",
    "3ÔºâOnce connected, click `\"Explorer\"` in the left sidebar > \"Open Folder\", and navigate to your home directory under the project grant:\n",
    "```bash\n",
    "/jet/home/<your_username>  #change <your_username> to your username\n",
    "```\n",
    "\n",
    "4ÔºâYou can now drag your notebook files directly into the right-hand pane (your remote home directory), or upload them using `scp` into your folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9-H9-x3TAOJ"
   },
   "source": [
    "> ‚ùóÔ∏è‚ö†Ô∏è The following steps should be executed in the **VSCode integrated terminal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUztKF5HTAOJ"
   },
   "source": [
    "#### **1.2 Navigate to Your Directory**\n",
    "Make sure to use this `/jet/home/<your_username>` as your working directory, since all subsequent operations (up to submission) are based on this path.\n",
    "```bash\n",
    "cd /jet/home/<your_username>  #change <your_username> to your username\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZ35Ee4sTAOJ"
   },
   "source": [
    "#### **1.3 Request a Compute Node**\n",
    "```bash\n",
    "interact -p GPU-shared --gres=gpu:v100-32:1 -t 8:00:00 -A cis250019p\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0deDw_CkTAOJ"
   },
   "source": [
    "#### **1.4 Load the Anaconda Module**\n",
    "```bash\n",
    "module load anaconda3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4B-lvnNJTAOJ"
   },
   "source": [
    "#### **1.5 Activate the provided HW Environment**\n",
    "```bash\n",
    "conda deactivate # First, deactivate any existing Conda environment\n",
    "conda activate /ocean/projects/cis250019p/mzhang23/TA/envs/IDLS26 && export PYTHONNOUSERSITE=1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWmfw14NTAOK"
   },
   "source": [
    "#### **1.6 Start Jupyter Notebook**\n",
    "Launch Jupyter Notebook:\n",
    "```bash\n",
    "jupyter notebook --no-browser --ip=0.0.0.0\n",
    "```\n",
    "\n",
    "Go to **Kernel** ‚Üí **Select Another Kernel** ‚Üí **Existing Jupyter Server**\n",
    "   Enter the URL of the Jupyter Server:```http://{hostname}:{port}/tree?token={token}```\n",
    "   \n",
    "   *(Usually, this URL appears in the terminal output after you run `jupyter notebook --no-browser --ip=0.0.0.0`, in a line like:  ‚ÄúJupyter Server is running at: http://...‚Äù)*\n",
    "\n",
    "   - eg: `http://v011.ib.bridges2.psc.edu:8888/tree?token=e4b302434e68990f28bc2b4ae8d216eb87eecb7090526249`\n",
    "\n",
    "> **Note**: Replace `{hostname}`, `{port}` and `{token}` with your actual values from the Jupyter output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sejTPxc0TAOK"
   },
   "source": [
    "After launching the Jupyter notebook, you can run the cells directly inside the notebook ‚Äî no need to use the terminal for the remaining steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3VWeHyS3kG9"
   },
   "source": [
    "#### **1.7 Navigate to Your Jet Directory**\n",
    "\n",
    "After launching the Jupyter notebook, you can run the cells directly inside the notebook ‚Äî no need to use the terminal for the remaining steps.\n",
    "\n",
    "First, navigate to your **Jet directory** (`/jet/home/<your_username>`).\n",
    "\n",
    "Ô∏è‚ùóÔ∏è‚ö† Please make sure to use your **Jet directory**, not the **Ocean path** ‚Äî **all HW setup and outputs below are based on this directory**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOnI9xHM3kHC"
   },
   "outputs": [],
   "source": [
    "# Make sure you are in your directory\n",
    "!pwd #should be /jet/home/<your_username>, if not, uncomment the following line and replace with your actual username:\n",
    "%cd /jet/home/<your_username>\n",
    "#TODO: replace the \"<your_username>\" to yours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrdNB7VI3kHD"
   },
   "source": [
    "### Step 2: Configure Kaggle API Access\n",
    "\n",
    "‚ö†Ô∏è **Important:** You must provide your own Kaggle API credentials for data downloads, while creating your deliverables for code submission and (optionally) submitting to the competition.\n",
    "\n",
    "#### How to obtain your Kaggle API credentials\n",
    "\n",
    "1. Navigate to your **Kaggle Profile ‚Üí Settings** tab.\n",
    "2. Click **‚ÄúGenetate New Token‚Äù**\n",
    "3. Copy and paste the access token below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXxci9RA3kHD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "os.environ['KAGGLE_USERNAME'] = \"<your-username>\" # TODO: Verify in Settings\n",
    "os.environ['KAGGLE_API_TOKEN'] = \"<your-key>\" # TODO: Add Access Token (must be the new token starting with \"KGAT_\")\n",
    "\n",
    "# Verify\n",
    "import kaggle\n",
    "api = kaggle.api  # Already authenticated on import\n",
    "api.competitions_list_cli()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHgQ2mR_3kHD"
   },
   "source": [
    "### Step 3: Get Data\n",
    "\n",
    "‚ùóÔ∏è‚ö†Ô∏è In this homework, you need to download the dataset to the **GPU node‚Äôs local storage (`$LOCAL`)** instead of using the shared /ocean directory, in order to avoid I/O bottlenecks. Using the shared filesystem may slow down training drastically and can take hours per epoch.\n",
    "\n",
    "Note that **the local storage on a compute node is temporary and will be cleared** when your node time limit is reached or when you move to a different node. Therefore, **every time you run on a new node, you need to re-run the dataset download step**. However, as long as you stay on the same node, you do NOT need to download the dataset again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the HW2P2 kaggle competition (https://www.kaggle.com/t/12ca057eb27d4c62ae093a0f407e0651)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YNxFJYL3kHD"
   },
   "outputs": [],
   "source": [
    "api.competition_download_files('11785-hw-2-p-2-face-verification-spring-2026', path=os.environ[\"LOCAL\"])\n",
    "!mkdir -p $LOCAL/dataset\n",
    "!unzip -qo $LOCAL/11785-hw-2-p-2-face-verification-spring-2026.zip -d $LOCAL/dataset\n",
    "!rm -f $LOCAL/11785-hw-2-p-2-face-verification-spring-2026.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the following block to explore the shared directory structure:\n",
    "\n",
    "(Note: You do **NOT** need to change any `data_path` in the code below. Each time you start on a new node, as long as you run the dataset download block above, your dataset will always be located at:`/local/dataset/hw2p2_data`. This path will not change even if you are assigned to a different node.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = \"/local/dataset/hw2p2_data\" # this is the path of the dataset on your node\n",
    "print(\"Files in shared hw2p2 dataset:\", os.listdir(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install tree\n",
    "!tree -L 2 /local/dataset/hw2p2_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1XE_p9OsQGp"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:34:52.546352Z",
     "iopub.status.busy": "2026-01-03T17:34:52.545997Z",
     "iopub.status.idle": "2026-01-03T17:35:09.556100Z",
     "shell.execute_reply": "2026-01-03T17:35:09.555211Z",
     "shell.execute_reply.started": "2026-01-03T17:34:52.546316Z"
    },
    "id": "GKZw2YsvsPZV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "from torchvision.io import decode_image\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.v2 as T\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics as mt\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import glob\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_metric_learning import samplers\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OgkfYwP7HVt"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnmbQ6VntU5D"
   },
   "source": [
    "### ‚ö†Ô∏è `Note`\n",
    "\n",
    "- You will need to set the root path to your `hw2p2_data` folder in `data: root:`. This will depend on your setup. For eg. if you are following out setup instruction:\n",
    "  - `Colab:`: `\"/content/dataset/hw2p2_data\"`\n",
    "  - `Kaggle:`: `\"/kaggle/input/11785-hw-2-p-2-face-verification-spring-2026/hw2p2_data\"`\n",
    "  - `PSC`: `\"/local/dataset/hw2p2_data\"`\n",
    "\n",
    "Kindly modify your configurations to suit your ablations and be keen to include your name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:35:18.983391Z",
     "iopub.status.busy": "2026-01-03T17:35:18.982564Z",
     "iopub.status.idle": "2026-01-03T17:35:18.987725Z",
     "shell.execute_reply": "2026-01-03T17:35:18.986865Z",
     "shell.execute_reply.started": "2026-01-03T17:35:18.983350Z"
    },
    "id": "CMXkHmFc7G9m",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data_root': \"<ENTER-YOUR-DATA-ROOT-PATH-HERE>\", # TODO: Add your data root\n",
    "    'batch_size': 64, # Increase this if your GPU can handle it\n",
    "    'lr': 0.01,\n",
    "    'epochs': 2, # 20 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.\n",
    "    'num_classes': 8631, # Dataset contains 8631 classes for classification, reduce this number if you want to train on a subset, but only for train dataset and not on val dataset\n",
    "    'checkpoint_dir': \"<ENTER-YOUR-CHECKPOINT-DIR-HERE>\", #TODO: Checkpoint directory\n",
    "    'augument': True\n",
    "    # Include other parameters as needed.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4QjjC6xnMFs"
   },
   "source": [
    "# Data Augumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:35:23.974444Z",
     "iopub.status.busy": "2026-01-03T17:35:23.974103Z",
     "iopub.status.idle": "2026-01-03T17:35:23.981418Z",
     "shell.execute_reply": "2026-01-03T17:35:23.980618Z",
     "shell.execute_reply.started": "2026-01-03T17:35:23.974414Z"
    },
    "id": "x-jrgnbQyR2s",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_transforms(image_size: int = 112, augment: bool = True) -> T.Compose:\n",
    "    \"\"\"Create transform pipeline for face recognition.\"\"\"\n",
    "\n",
    "    # Step 1: Basic transformations\n",
    "    transform_list = [\n",
    "        # Resize the image to the desired size (image_size x image_size)\n",
    "        T.Resize((image_size, image_size)),\n",
    "\n",
    "        # Convert PIL Image to tensor\n",
    "        T.ToTensor(),\n",
    "\n",
    "        # Convert image to float32 and scale the pixel values to [0, 1]\n",
    "        T.ToDtype(torch.float32, scale=True),\n",
    "    ]\n",
    "\n",
    "    # Step 2: Data augmentation (optional, based on `augment` argument)\n",
    "    if augment:  # This block will be executed if `augment=True`\n",
    "        # TODO: Add transformations for data augmentation (e.g., random horizontal flip, rotation, etc.)\n",
    "        # HINT: What transforms help faces look more varied?\n",
    "        # Think: Does a horizontally flipped face still look like the same person?\n",
    "        # What about small rotations or color changes?\n",
    "        # Example:\n",
    "        transform_list.extend([\n",
    "            # Add your augmentations here\n",
    "        ])\n",
    "\n",
    "    # Step 3: Standard normalization for image recognition tasks\n",
    "    # The Normalize transformation requires mean and std values for each channel (R, G, B).\n",
    "    # Here, we are normalizing the pixel values to have a mean of 0.5 and std of 0.5 for each channel.\n",
    "    transform_list.extend([\n",
    "        T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Standard mean and std for face recognition tasks\n",
    "    ])\n",
    "\n",
    "    # Return the composed transformation pipeline\n",
    "    return T.Compose(transform_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:36:09.081676Z",
     "iopub.status.busy": "2026-01-03T17:36:09.081353Z",
     "iopub.status.idle": "2026-01-03T17:36:09.087819Z",
     "shell.execute_reply": "2026-01-03T17:36:09.086868Z",
     "shell.execute_reply.started": "2026-01-03T17:36:09.081649Z"
    },
    "id": "f3d1tsYE0pb-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_transforms = create_transforms(augment=config['augument'])\n",
    "val_transforms   = create_transforms(augment=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEAW65sB8Wlp"
   },
   "source": [
    "# Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzxNNzGe8ccB"
   },
   "source": [
    "## Classification Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:36:01.981574Z",
     "iopub.status.busy": "2026-01-03T17:36:01.981187Z",
     "iopub.status.idle": "2026-01-03T17:36:02.215976Z",
     "shell.execute_reply": "2026-01-03T17:36:02.215106Z",
     "shell.execute_reply.started": "2026-01-03T17:36:01.981542Z"
    },
    "id": "p6Nn57B-7F-i",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Custom dataset for loading images with optional labels.\"\"\"\n",
    "\n",
    "    def __init__(self, root, transform, num_classes=None, preload=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str): Dataset root directory.\n",
    "                Expected structure:\n",
    "                  - labeled:\n",
    "                        root/\n",
    "                          images/\n",
    "                          labels.txt   (img label)\n",
    "                  - unlabeled:\n",
    "                        root/\n",
    "                          images/\n",
    "            transform (callable): Transform applied to images.\n",
    "            num_classes (int, optional): Number of classes to keep (labeled only).\n",
    "            preload (bool): Preload images into memory.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.preload = preload\n",
    "\n",
    "        self.image_paths = []\n",
    "        self.labels = None      # None ‚Üí unlabeled dataset\n",
    "        self.classes = None\n",
    "        self.images = []\n",
    "\n",
    "        labels_file = os.path.join(self.root, \"labels.txt\")\n",
    "        images_dir = os.path.join(self.root, \"images\")\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # Detect labeled vs unlabeled dataset\n",
    "        # --------------------------------------------------\n",
    "        has_labels = os.path.exists(labels_file)\n",
    "\n",
    "        if has_labels:\n",
    "            self.labels = []\n",
    "            self.classes = set()\n",
    "\n",
    "            with open(labels_file, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            # Sort by label (keeps your original behavior)\n",
    "            lines = sorted(lines, key=lambda x: int(x.strip().split(\" \")[-1]))\n",
    "\n",
    "            all_labels = sorted(set(int(line.strip().split(\" \")[1]) for line in lines))\n",
    "\n",
    "            if num_classes is not None:\n",
    "                selected_classes = set(all_labels[:num_classes])\n",
    "            else:\n",
    "                selected_classes = set(all_labels)\n",
    "\n",
    "            for line in tqdm(lines, desc=\"Loading labeled dataset\"):\n",
    "                img_path, label = line.strip().split(\" \")\n",
    "                label = int(label)\n",
    "\n",
    "                if label in selected_classes:\n",
    "                    self.image_paths.append(os.path.join(images_dir, img_path))\n",
    "                    self.labels.append(label)\n",
    "                    self.classes.add(label)\n",
    "\n",
    "            self.classes = sorted(self.classes)\n",
    "\n",
    "            assert len(self.image_paths) == len(self.labels), \"Images and labels mismatch!\"\n",
    "\n",
    "        else:\n",
    "            # --------------------------------------------------\n",
    "            # Unlabeled dataset: load all images from images/\n",
    "            # --------------------------------------------------\n",
    "            image_files = sorted(os.listdir(images_dir))\n",
    "\n",
    "            for img in tqdm(image_files, desc=\"Loading unlabeled dataset\"):\n",
    "                self.image_paths.append(os.path.join(images_dir, img))\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # Preload images if requested\n",
    "        # --------------------------------------------------\n",
    "        if self.preload:\n",
    "            self.images = [\n",
    "                decode_image(p, mode=\"RGB\") / 255.0\n",
    "                for p in tqdm(self.image_paths, desc=\"Preloading images\")\n",
    "            ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.preload:\n",
    "            image = self.images[idx]\n",
    "        else:\n",
    "            image = decode_image(self.image_paths[idx], mode=\"RGB\") / 255.0\n",
    "\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Return with or without label\n",
    "        if self.labels is not None:\n",
    "            return image, self.labels[idx]\n",
    "        else:\n",
    "            return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:36:17.291445Z",
     "iopub.status.busy": "2026-01-03T17:36:17.291117Z",
     "iopub.status.idle": "2026-01-03T17:36:18.960937Z",
     "shell.execute_reply": "2026-01-03T17:36:18.960063Z",
     "shell.execute_reply.started": "2026-01-03T17:36:17.291416Z"
    },
    "id": "GOJOrQLX02Gh",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "cls_data_dir = config['data_root'] + '/cls_data'\n",
    "\n",
    "# Datasets\n",
    "# TODO: Implement the datasets for training, validation, and testing\n",
    "cls_train_dataset = NotImplementedError\n",
    "cls_val_dataset   = NotImplementedError\n",
    "cls_test_dataset  = NotImplementedError\n",
    "\n",
    "assert cls_train_dataset.classes == cls_val_dataset.classes == cls_test_dataset.classes, \"Class mismatch!\"\n",
    "\n",
    "# Dataloaders\n",
    "# TODO: Implement the dataloaders for training, validation, and testing\n",
    "cls_train_loader = NotImplementedError\n",
    "cls_val_loader   = NotImplementedError\n",
    "cls_test_loader  = NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPSk8DyK8htk"
   },
   "source": [
    "## Verification Dataset and Datatloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:36:26.340456Z",
     "iopub.status.busy": "2026-01-03T17:36:26.339759Z",
     "iopub.status.idle": "2026-01-03T17:36:26.348372Z",
     "shell.execute_reply": "2026-01-03T17:36:26.347579Z",
     "shell.execute_reply.started": "2026-01-03T17:36:26.340423Z"
    },
    "id": "KBleUieO8lwG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImagePairDataset(Dataset):\n",
    "    \"\"\"Custom dataset for loading and transforming image pairs (labeled or unlabeled).\"\"\"\n",
    "\n",
    "    def __init__(self, root, pairs_file, transform, preload=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str): Path to the directory containing the images.\n",
    "            pairs_file (str): File with:\n",
    "                - img1 img2 label   (labeled)\n",
    "                - img1 img2         (unlabeled)\n",
    "            transform (callable): Transform to apply to images.\n",
    "            preload (bool): Whether to preload images into memory.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.preload = preload\n",
    "\n",
    "        self.image1_paths = []\n",
    "        self.image2_paths = []\n",
    "        self.matches = None  # None ‚Üí unlabeled dataset\n",
    "\n",
    "        # Read file\n",
    "        with open(pairs_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Detect whether labels exist\n",
    "        first_cols = lines[0].strip().split()\n",
    "        has_labels = len(first_cols) == 3\n",
    "\n",
    "        if has_labels:\n",
    "            self.matches = []\n",
    "\n",
    "        for line in tqdm(lines, desc=\"Loading dataset\"):\n",
    "            parts = line.strip().split()\n",
    "\n",
    "            if has_labels:\n",
    "                img1, img2, match = parts\n",
    "                self.matches.append(int(match))\n",
    "            else:\n",
    "                img1, img2 = parts\n",
    "\n",
    "            self.image1_paths.append(os.path.join(self.root, img1))\n",
    "            self.image2_paths.append(os.path.join(self.root, img2))\n",
    "\n",
    "        assert len(self.image1_paths) == len(self.image2_paths)\n",
    "        if has_labels:\n",
    "            assert len(self.matches) == len(self.image1_paths)\n",
    "\n",
    "        # Preload images if requested\n",
    "        if self.preload:\n",
    "            self.image1_cache = [\n",
    "                decode_image(p, mode=\"RGB\") / 255.0\n",
    "                for p in tqdm(self.image1_paths, desc=\"Preloading image1\")\n",
    "            ]\n",
    "            self.image2_cache = [\n",
    "                decode_image(p, mode=\"RGB\") / 255.0\n",
    "                for p in tqdm(self.image2_paths, desc=\"Preloading image2\")\n",
    "            ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image1_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load images\n",
    "        if self.preload:\n",
    "            img1 = self.image1_cache[idx]\n",
    "            img2 = self.image2_cache[idx]\n",
    "        else:\n",
    "            img1 = decode_image(self.image1_paths[idx], mode=\"RGB\") / 255.0\n",
    "            img2 = decode_image(self.image2_paths[idx], mode=\"RGB\") / 255.0\n",
    "\n",
    "        img1 = self.transform(img1)\n",
    "        img2 = self.transform(img2)\n",
    "\n",
    "        # Return with or without label\n",
    "        if self.matches is not None:\n",
    "            return img1, img2, self.matches[idx]\n",
    "        else:\n",
    "            return img1, img2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:36:54.254463Z",
     "iopub.status.busy": "2026-01-03T17:36:54.253634Z",
     "iopub.status.idle": "2026-01-03T17:38:23.795794Z",
     "shell.execute_reply": "2026-01-03T17:38:23.795110Z",
     "shell.execute_reply.started": "2026-01-03T17:36:54.254432Z"
    },
    "id": "-ALvOc6L2r3d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "ver_data_dir = config['data_root'] + '/ver_data'\n",
    "\n",
    "# Datasets\n",
    "# TODO: Implement the datasets for validation and testing\n",
    "ver_val_dataset  = NotImplementedError\n",
    "ver_test_dataset = NotImplementedError\n",
    "\n",
    "# Dataloader\n",
    "# TODO: Implement the dataloaders for validation and testing\n",
    "ver_val_loader   = NotImplementedError\n",
    "ver_test_loader  = NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "436KzM6u-3A2"
   },
   "source": [
    "# EDA and Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:38:29.566090Z",
     "iopub.status.busy": "2026-01-03T17:38:29.565294Z",
     "iopub.status.idle": "2026-01-03T17:38:29.662144Z",
     "shell.execute_reply": "2026-01-03T17:38:29.661441Z",
     "shell.execute_reply.started": "2026-01-03T17:38:29.566056Z"
    },
    "id": "AhnoHopx-0RB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Double-check your dataset/dataloaders work as expected\n",
    "\n",
    "print(\"Number of classes    : \", len(cls_train_dataset.classes))\n",
    "print(\"No. of train images  : \", cls_train_dataset.__len__())\n",
    "print(\"Shape of image       : \", cls_train_dataset[0][0].shape)\n",
    "print(\"Batch size           : \", config['batch_size'])\n",
    "print(\"Train batches        : \", cls_train_loader.__len__())\n",
    "print(\"Val batches          : \", cls_val_loader.__len__())\n",
    "\n",
    "# Feel free to print more things if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niSJ49lzpHgW"
   },
   "source": [
    "### Classification Dataset Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:38:36.551291Z",
     "iopub.status.busy": "2026-01-03T17:38:36.550737Z",
     "iopub.status.idle": "2026-01-03T17:38:41.516979Z",
     "shell.execute_reply": "2026-01-03T17:38:41.515964Z",
     "shell.execute_reply.started": "2026-01-03T17:38:36.551258Z"
    },
    "id": "fK3t65VU5Qlz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def show_cls_dataset_samples(train_loader, val_loader, test_loader, samples_per_set=8, figsize=(10, 6)):\n",
    "    \"\"\"\n",
    "    Display samples from train, validation, and test datasets side by side\n",
    "\n",
    "    Args:\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        test_loader: Test data loader\n",
    "        samples_per_set: Number of samples to show from each dataset\n",
    "        figsize: Figure size (width, height)\n",
    "    \"\"\"\n",
    "    def denormalize(x):\n",
    "        \"\"\"Denormalize images from [-1, 1] to [0, 1]\"\"\"\n",
    "        return x * 0.5 + 0.5\n",
    "\n",
    "    def get_samples(loader, n):\n",
    "        \"\"\"Get n samples from a dataloader\"\"\"\n",
    "        batch = next(iter(loader))\n",
    "        return batch[0][:n], batch[1][:n]\n",
    "\n",
    "    # Get samples from each dataset\n",
    "    train_imgs, train_labels = get_samples(train_loader, samples_per_set)\n",
    "    val_imgs, val_labels = get_samples(val_loader, samples_per_set)\n",
    "    test_imgs, test_labels = get_samples(test_loader, samples_per_set)\n",
    "\n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(3, 1, figsize=figsize)\n",
    "\n",
    "    # Plot each dataset\n",
    "    for idx, (imgs, labels, title) in enumerate([\n",
    "        (train_imgs, train_labels, 'Training Samples'),\n",
    "        (val_imgs, val_labels, 'Validation Samples'),\n",
    "        (test_imgs, test_labels, 'Test Samples')\n",
    "    ]):\n",
    "\n",
    "        # Create grid of images\n",
    "        grid = make_grid(denormalize(imgs), nrow=8, padding=2)\n",
    "\n",
    "        # Display grid\n",
    "        axes[idx].imshow(grid.permute(1, 2, 0).cpu())\n",
    "        axes[idx].axis('off')\n",
    "        axes[idx].set_title(title, fontsize=10)\n",
    "\n",
    "        # Add class labels below images (with smaller font)\n",
    "        grid_width = grid.shape[2]\n",
    "        imgs_per_row = min(8, samples_per_set)\n",
    "        img_width = grid_width // imgs_per_row\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            col = i % imgs_per_row  # Calculate column position\n",
    "            if label<len(train_loader.dataset.classes):\n",
    "              class_name = train_loader.dataset.classes[label]\n",
    "            else:\n",
    "              class_name = f\"Class {label} (Unknown)\"\n",
    "            axes[idx].text(col * img_width + img_width/2,\n",
    "                         grid.shape[1] + 5,\n",
    "                         class_name,\n",
    "                         ha='center',\n",
    "                         va='top',\n",
    "                         fontsize=6,\n",
    "                         rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_cls_dataset_samples(cls_train_loader, cls_val_loader, cls_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZPVGXOu59Wh"
   },
   "source": [
    "### Ver Dataset Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:38:49.868759Z",
     "iopub.status.busy": "2026-01-03T17:38:49.868052Z",
     "iopub.status.idle": "2026-01-03T17:38:51.242800Z",
     "shell.execute_reply": "2026-01-03T17:38:51.241937Z",
     "shell.execute_reply.started": "2026-01-03T17:38:49.868722Z"
    },
    "id": "6pBN7Z9K5iAM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def show_ver_dataset_samples(val_loader, samples_per_set=4, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Display verification pairs from the validation dataset\n",
    "\n",
    "    Args:\n",
    "        val_loader: Validation data loader\n",
    "        samples_per_set: Number of pairs to show from the dataset\n",
    "        figsize: Figure size (width, height)\n",
    "    \"\"\"\n",
    "    def denormalize(x):\n",
    "        \"\"\"Denormalize images from [-1, 1] to [0, 1]\"\"\"\n",
    "        return x * 0.5 + 0.5\n",
    "\n",
    "    def get_samples(loader, n):\n",
    "        \"\"\"Get n samples from a dataloader\"\"\"\n",
    "        batch = next(iter(loader))\n",
    "        return batch[0][:n], batch[1][:n], batch[2][:n]\n",
    "\n",
    "    # Get samples from the validation dataset\n",
    "    val_imgs1, val_imgs2, val_labels = get_samples(val_loader, samples_per_set)\n",
    "\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    # Create grids for both images in each pair\n",
    "    grid1 = make_grid(denormalize(val_imgs1), nrow=samples_per_set, padding=2)\n",
    "    grid2 = make_grid(denormalize(val_imgs2), nrow=samples_per_set, padding=2)\n",
    "\n",
    "    # Combine the grids vertically\n",
    "    combined_grid = torch.cat([grid1, grid2], dim=1)\n",
    "\n",
    "    # Display the combined grid\n",
    "    ax.imshow(combined_grid.permute(1, 2, 0).cpu())\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Validation Pairs', fontsize=10)\n",
    "\n",
    "    # Determine dimensions for placing the labels\n",
    "    grid_width = grid1.shape[2]\n",
    "    img_width = grid_width // samples_per_set\n",
    "\n",
    "    # Add match/non-match labels for each pair\n",
    "    for i, label in enumerate(val_labels):\n",
    "        match_text = \"‚úì Match\" if label == 1 else \"‚úó Non-match\"\n",
    "        color = 'green' if label == 1 else 'red'\n",
    "\n",
    "        # Define a background box for the label\n",
    "        bbox_props = dict(\n",
    "            boxstyle=\"round,pad=0.3\",\n",
    "            fc=\"white\",\n",
    "            ec=color,\n",
    "            alpha=0.8\n",
    "        )\n",
    "\n",
    "        ax.text(i * img_width + img_width / 2,\n",
    "                combined_grid.shape[1] + 15,  # Position below the images\n",
    "                match_text,\n",
    "                ha='center',\n",
    "                va='top',\n",
    "                fontsize=8,\n",
    "                color=color,\n",
    "                bbox=bbox_props)\n",
    "\n",
    "    plt.suptitle(\"Verification Pairs (Top: Image 1, Bottom: Image 2)\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.05)\n",
    "    plt.show()\n",
    "\n",
    "show_ver_dataset_samples(ver_val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3TUocDw_JU_"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4pZEjs2peC4"
   },
   "source": [
    "## FAQ\n",
    "\n",
    "### **What is the ‚Äúvery low early-deadline architecture‚Äù (mandatory early submission)?**\n",
    "\n",
    "The *very low early-deadline architecture* is a simple **5-layer convolutional neural network (CNN)** designed to meet the early-submission requirements. Remember that the **maximum parameter budget for this homework is 30 million parameters**.\n",
    "\n",
    "**Architecture details:**\n",
    "\n",
    "* The network consists of **5 convolutional layers**.\n",
    "* The **first convolutional layer**:\n",
    "\n",
    "  * Output channels: **64**\n",
    "  * Kernel size: **7**\n",
    "  * Stride: **4**\n",
    "* The **next four convolutional layers** have:\n",
    "\n",
    "  * Output channels: **128 ‚Üí 256 ‚Üí 512 ‚Üí 1024**\n",
    "  * Kernel size: **3**\n",
    "  * Stride: **2**\n",
    "* Each convolutional layer is followed by:\n",
    "\n",
    "  * **Batch Normalization**\n",
    "  * **ReLU activation**\n",
    "\n",
    "For creating convolutional layers, refer to the PyTorch documentation:\n",
    "[https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
    "\n",
    "**Strided convolutions and padding:**\n",
    "\n",
    "* Recall from lecture that a strided convolution can be viewed as a stride-1 convolution followed by downsampling.\n",
    "* To preserve spatial alignment when using strided convolutions, choose:\n",
    "\n",
    "  ```\n",
    "  padding = kernel_size // 2\n",
    "  ```\n",
    "* Think about *why* this works: symmetric padding ensures the convolution remains centered over the input.\n",
    "\n",
    "**Final pooling and flattening:**\n",
    "\n",
    "* After the convolutional layers, apply **Adaptive Average Pooling** to reduce the spatial dimensions to **1 √ó 1**:\n",
    "\n",
    "  * Use `AdaptiveAvgPool2d((1, 1))`\n",
    "  * Documentation: [https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html)\n",
    "* Finally, remove the trivial spatial dimensions (e.g., using `Flatten`).\n",
    "\n",
    "For more layer options, see:\n",
    "[https://pytorch.org/docs/stable/nn.html](https://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "---\n",
    "\n",
    "### **Why does this simple network use multiple downsampling convolutions?**\n",
    "\n",
    "The input images are **112 √ó 112**. Each convolutional layer performs spatial downsampling.\n",
    "\n",
    "* Downsampling by **2√ó** effectively **doubles the receptive field**, allowing neurons to capture information from a larger region of the image.\n",
    "* Downsampling by **32√ó** in total is standard in many modern image models and provides a good balance between spatial resolution and semantic abstraction.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why does this simple network use large channel sizes?**\n",
    "\n",
    "Each time the spatial resolution is downsampled by **2√ó**, the computational cost drops by **4√ó** (assuming the number of channels stays constant).\n",
    "\n",
    "To compensate:\n",
    "\n",
    "* The number of channels is increased by **2√ó**, which increases computation by **4√ó**\n",
    "* This keeps the overall computation roughly balanced across layers\n",
    "\n",
    "Another intuition:\n",
    "\n",
    "* As spatial resolution decreases, some spatial information is lost\n",
    "* Increasing the number of channels helps preserve this information in the **feature (channel) dimension**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:39:01.870759Z",
     "iopub.status.busy": "2026-01-03T17:39:01.869928Z",
     "iopub.status.idle": "2026-01-03T17:39:03.473354Z",
     "shell.execute_reply": "2026-01-03T17:39:03.472608Z",
     "shell.execute_reply.started": "2026-01-03T17:39:01.870720Z"
    },
    "id": "4LLX2Rki_LzA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple 5-layer CNN backbone for face classification and feature extraction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.backbone = torch.nn.Sequential(\n",
    "            # TODO: Implement 5-layer CNN as described in FAQ for early submission\n",
    "        )\n",
    "        self.cls_layer = NotImplementedError # TODO:  Final classification layer\n",
    "\n",
    "    def forward(self, x, return_feats: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input image batch of shape (B, 3, H, W)\n",
    "            return_feats (bool): If True, return only feature embeddings\n",
    "\n",
    "        Returns:\n",
    "            dict with keys:\n",
    "                - 'feats': feature embeddings\n",
    "                - 'out': classification logits\n",
    "        \"\"\"\n",
    "        feats = self.backbone(x)\n",
    "        out = self.cls_layer(feats)\n",
    "        return {\"feats\": feats, \"out\": out}\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = Network(num_classes=config[\"num_classes\"]).to(DEVICE)\n",
    "\n",
    "# Optional: model summary\n",
    "summary(model, (3, 112, 112))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6L9AffxhubP8"
   },
   "source": [
    "# Loss | Optimizer | Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:39:09.502104Z",
     "iopub.status.busy": "2026-01-03T17:39:09.501341Z",
     "iopub.status.idle": "2026-01-03T17:39:09.508048Z",
     "shell.execute_reply": "2026-01-03T17:39:09.507252Z",
     "shell.execute_reply.started": "2026-01-03T17:39:09.502072Z"
    },
    "id": "pDP--pND_3Vy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Loss Function\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Cross-entropy loss is standard for multi-class classification.\n",
    "# Label smoothing can help regularization when the number of classes is large.\n",
    "# TODO: Initialize the criterion properly\n",
    "criterion = NotImplementedError\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Optimizer\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# SGD with momentum is a strong baseline for CNN training.\n",
    "# Weight decay provides L2 regularization and helps prevent overfitting.\n",
    "\n",
    "# TODO: Initialize the optimizer properly\n",
    "optimizer = NotImplementedError\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Learning Rate Scheduler\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Cosine annealing smoothly decays the learning rate to zero over training.\n",
    "# TODO: Initialize the scheduler properly\n",
    "scheduler = NotImplementedError\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Mixed-Precision Training (FP16)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# GradScaler enables stable mixed-precision training on supported GPUs (e.g., T4, V100).\n",
    "# This reduces memory usage and can significantly speed up training.\n",
    "scaler = torch.amp.GradScaler(device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-d5ZDQfpw7gR"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:47:04.511282Z",
     "iopub.status.busy": "2026-01-03T17:47:04.510864Z",
     "iopub.status.idle": "2026-01-03T17:47:04.516490Z",
     "shell.execute_reply": "2026-01-03T17:47:04.515915Z",
     "shell.execute_reply.started": "2026-01-03T17:47:04.511250Z"
    },
    "id": "7Ecg0J2sw9jJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Tracks and computes the running average of a scalar metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0.0\n",
    "        self.avg = 0.0\n",
    "        self.sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n: int = 1):\n",
    "        self.val = float(val)\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / max(self.count, 1)\n",
    "\n",
    "\n",
    "def topk_accuracy(logits: torch.Tensor, targets: torch.Tensor, topk=(1,)):\n",
    "    \"\"\"\n",
    "    Computes top-k accuracy for the given logits and targets.\n",
    "\n",
    "    Args:\n",
    "        logits (Tensor): Model outputs of shape (B, C)\n",
    "        targets (Tensor): Ground-truth labels of shape (B,)\n",
    "        topk (tuple): Values of k for top-k accuracy\n",
    "\n",
    "    Returns:\n",
    "        List[Tensor]: Accuracy values in percentage for each k\n",
    "    \"\"\"\n",
    "    maxk = min(max(topk), logits.size(1))\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    _, preds = logits.topk(maxk, dim=1, largest=True, sorted=True)\n",
    "    preds = preds.t()\n",
    "    correct = preds.eq(targets.view(1, -1))\n",
    "\n",
    "    accuracies = []\n",
    "    for k in topk:\n",
    "        k = min(k, maxk)\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        accuracies.append(correct_k * 100.0 / batch_size)\n",
    "\n",
    "    return accuracies\n",
    "\n",
    "\n",
    "def verification_metrics(labels, scores, fpr_targets=None):\n",
    "    \"\"\"\n",
    "    Computes standard verification metrics: ACC, EER, AUC, and TPR@FPR.\n",
    "\n",
    "    Args:\n",
    "        labels (list or array): Binary labels (0 or 1)\n",
    "        scores (list or array): Similarity scores\n",
    "        fpr_targets (list, optional): Target FPR values for reporting TPR\n",
    "\n",
    "    Returns:\n",
    "        dict: Verification metrics\n",
    "    \"\"\"\n",
    "    labels = np.asarray(labels)\n",
    "    scores = np.asarray(scores)\n",
    "\n",
    "    # TODO: Compute verificaion metrics\n",
    "    # Hint: Read the documentation for sklearn.metrics.roc_curve\n",
    "    # and use it to compute fpr and tpr\n",
    "    fpr, tpr, _ = NotImplementedError\n",
    "\n",
    "    # Hint: You can use interp1d from scipy.interpolate to create roc_interp\n",
    "    roc_interp = interp1d(fpr, tpr, bounds_error=False, fill_value=(0.0, 1.0))\n",
    "\n",
    "    # Equal Error Rate (EER)\n",
    "    # Hint: You might find brentq from scipy.optimize helpful to compute EER\n",
    "    eer = NotImplementedError\n",
    "\n",
    "    # Area Under Curve (AUC)\n",
    "    # Hint: Use mt.auc to compute AUC\n",
    "    # Be careful to multiply by 100.0 to express as percentage\n",
    "    auc = NotImplementedError\n",
    "\n",
    "    # Accuracy (best threshold)\n",
    "    # TODO: Compute TNR, pos_count, neg_count, and accuracy\n",
    "    # Hint: Use fpr and tpr to compute TNR\n",
    "    # Hint: pos_count is the number of positive labels (1s)\n",
    "    # Hint: neg_count is the number of negative labels (0s)\n",
    "    tnr = NotImplementedError\n",
    "    pos_count = NotImplementedError\n",
    "    neg_count = NotImplementedError\n",
    "    acc = NotImplementedError\n",
    "\n",
    "    # TPR @ specific FPRs\n",
    "    tpr_at_fpr = []\n",
    "    if fpr_targets is not None:\n",
    "        for fpr_val in fpr_targets:\n",
    "            tpr_at_fpr.append(\n",
    "                (f\"TPR@FPR={fpr_val}\", 100.0 * float(roc_interp(fpr_val)))\n",
    "            )\n",
    "\n",
    "    return {\n",
    "        \"ACC\": acc,\n",
    "        \"EER\": eer,\n",
    "        \"AUC\": auc,\n",
    "        \"TPRs\": tpr_at_fpr,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juUbZnP0AEUi"
   },
   "source": [
    "# Train and Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:47:42.901091Z",
     "iopub.status.busy": "2026-01-03T17:47:42.900726Z",
     "iopub.status.idle": "2026-01-03T17:47:42.910028Z",
     "shell.execute_reply": "2026-01-03T17:47:42.909369Z",
     "shell.execute_reply.started": "2026-01-03T17:47:42.901061Z"
    },
    "id": "IMnxvQT-AHsu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    scaler,\n",
    "    device,\n",
    "    criterion,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs one training epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to train\n",
    "        dataloader (DataLoader): Training dataloader\n",
    "        optimizer (Optimizer): Optimizer\n",
    "        scheduler (LRScheduler or None): Learning rate scheduler\n",
    "        scaler (GradScaler): AMP gradient scaler\n",
    "        device (torch.device): Training device\n",
    "        criterion (callable): Loss function\n",
    "\n",
    "    Returns:\n",
    "        tuple: (avg_accuracy, avg_loss)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    loss_meter = AverageMeter()\n",
    "    acc_meter = AverageMeter()\n",
    "\n",
    "    progress = tqdm(\n",
    "        dataloader,\n",
    "        desc=\"Train\",\n",
    "        dynamic_ncols=True,\n",
    "        leave=False,\n",
    "    )\n",
    "\n",
    "    for images, labels in progress:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Move data to device\n",
    "        images = images.to(device, non_blocking=True)\n",
    "\n",
    "        if isinstance(labels, (tuple, list)):\n",
    "            # For mixup / cutmix style labels\n",
    "            targets1, targets2, lam = labels\n",
    "            labels = (\n",
    "                targets1.to(device, non_blocking=True),\n",
    "                targets2.to(device, non_blocking=True),\n",
    "                lam,\n",
    "            )\n",
    "        else:\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        # Forward pass (mixed precision)\n",
    "        with torch.amp.autocast(device_type = 'cuda'):\n",
    "            outputs = model(images)\n",
    "            logits = outputs[\"out\"]\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward + optimizer step (AMP-safe)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Metrics\n",
    "        batch_loss = loss.item()\n",
    "        loss_meter.update(batch_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_acc = topk_accuracy(logits, labels, topk=(1,))[0].item()\n",
    "            acc_meter.update(batch_acc)\n",
    "\n",
    "        # Progress bar update\n",
    "        progress.set_postfix(\n",
    "            loss=f\"{batch_loss:.4f} ({loss_meter.avg:.4f})\",\n",
    "            acc=f\"{batch_acc:.2f}% ({acc_meter.avg:.2f}%)\",\n",
    "            lr=f\"{optimizer.param_groups[0]['lr']:.6f}\",\n",
    "        )\n",
    "\n",
    "    # Step scheduler once per epoch (for epoch-based schedulers)\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    return acc_meter.avg, loss_meter.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:47:55.426131Z",
     "iopub.status.busy": "2026-01-03T17:47:55.425355Z",
     "iopub.status.idle": "2026-01-03T17:47:55.432281Z",
     "shell.execute_reply": "2026-01-03T17:47:55.431647Z",
     "shell.execute_reply.started": "2026-01-03T17:47:55.426095Z"
    },
    "id": "5qkdH295wNUX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_epoch_cls(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device,\n",
    "    criterion,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs one validation epoch for classification.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model\n",
    "        dataloader (DataLoader): Validation dataloader\n",
    "        device (torch.device): Evaluation device\n",
    "        criterion (callable): Loss function\n",
    "\n",
    "    Returns:\n",
    "        tuple: (avg_accuracy, avg_loss)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    loss_meter = AverageMeter()\n",
    "    acc_meter = AverageMeter()\n",
    "\n",
    "    progress = tqdm(\n",
    "        dataloader,\n",
    "        desc=\"Val (Cls)\",\n",
    "        dynamic_ncols=True,\n",
    "        leave=False,\n",
    "    )\n",
    "\n",
    "    for images, labels in progress:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        # Forward pass (inference-only)\n",
    "        outputs = model(images)\n",
    "        logits = outputs[\"out\"]\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Metrics\n",
    "        batch_loss = loss.item()\n",
    "        loss_meter.update(batch_loss)\n",
    "\n",
    "        batch_acc = topk_accuracy(logits, labels, topk=(1,))[0].item()\n",
    "        acc_meter.update(batch_acc)\n",
    "\n",
    "        # Progress bar update\n",
    "        progress.set_postfix(\n",
    "            loss=f\"{batch_loss:.4f} ({loss_meter.avg:.4f})\",\n",
    "            acc=f\"{batch_acc:.2f}% ({acc_meter.avg:.2f}%)\",\n",
    "        )\n",
    "\n",
    "    return acc_meter.avg, loss_meter.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:47:59.245745Z",
     "iopub.status.busy": "2026-01-03T17:47:59.245129Z",
     "iopub.status.idle": "2026-01-03T17:47:59.538212Z",
     "shell.execute_reply": "2026-01-03T17:47:59.537415Z",
     "shell.execute_reply.started": "2026-01-03T17:47:59.245713Z"
    },
    "id": "-Yan5vLDyj-3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gc.collect() # These commands help you when you face CUDA OOM error\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q1gRMAsyknz"
   },
   "source": [
    "# Verification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:48:04.838535Z",
     "iopub.status.busy": "2026-01-03T17:48:04.837749Z",
     "iopub.status.idle": "2026-01-03T17:48:04.845740Z",
     "shell.execute_reply": "2026-01-03T17:48:04.845090Z",
     "shell.execute_reply.started": "2026-01-03T17:48:04.838486Z"
    },
    "id": "SSGeDCi-wa1W",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_epoch_ver(model, pair_dataloader, device, fpr_targets=None):\n",
    "    \"\"\"\n",
    "    Runs one validation epoch for verification (image pairs).\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model\n",
    "        pair_dataloader (DataLoader): DataLoader yielding (img1, img2, label) tuples\n",
    "        device (torch.device): Evaluation device\n",
    "        fpr_targets (list, optional): List of FPRs for TPR reporting\n",
    "\n",
    "    Returns:\n",
    "        dict: Verification metrics including ACC, EER, AUC, TPR@FPR\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    match_labels = []\n",
    "\n",
    "    progress = tqdm(\n",
    "        pair_dataloader,\n",
    "        desc=\"Val (Veri)\",\n",
    "        dynamic_ncols=True,\n",
    "        leave=False,\n",
    "    )\n",
    "\n",
    "    for images1, images2, labels in progress:\n",
    "        # Move data to device\n",
    "        images1 = images1.to(device, non_blocking=True)\n",
    "        images2 = images2.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        # Forward pass\n",
    "        images = torch.cat([images1, images2], dim=0)\n",
    "        outputs = model(images)\n",
    "        feats = F.normalize(outputs[\"feats\"], dim=1)\n",
    "\n",
    "        # Split features and compute similarity\n",
    "        feats1, feats2 = feats.chunk(2, dim=0)\n",
    "        similarity = F.cosine_similarity(feats1, feats2, dim=1)\n",
    "\n",
    "        # Accumulate results\n",
    "        scores.append(similarity.cpu().numpy())\n",
    "        match_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        progress.update()\n",
    "\n",
    "    # Concatenate batch-wise results\n",
    "    scores = np.concatenate(scores)\n",
    "    match_labels = np.concatenate(match_labels)\n",
    "\n",
    "    # Default FPR targets if not provided\n",
    "    if fpr_targets is None:\n",
    "        fpr_targets = [1e-4, 5e-4, 1e-3, 5e-3, 5e-2]\n",
    "\n",
    "    # Compute verification metrics\n",
    "    metric_dict = verification_metrics(match_labels, scores, fpr_targets)\n",
    "\n",
    "    # Optional: print metrics\n",
    "    print(\"Verification Metrics:\", metric_dict)\n",
    "\n",
    "    return metric_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piblCbe5yotj"
   },
   "source": [
    "# WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:48:08.746822Z",
     "iopub.status.busy": "2026-01-03T17:48:08.746500Z",
     "iopub.status.idle": "2026-01-03T17:48:16.376937Z",
     "shell.execute_reply": "2026-01-03T17:48:16.376252Z",
     "shell.execute_reply.started": "2026-01-03T17:48:08.746791Z"
    },
    "id": "HTIkCXBQyoM0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.login(key=\"<ENTER-YOUR-WANDB-API-KEY-HERE>\") # API Key is in your wandb account, under settings (wandb.ai/settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:48:21.764283Z",
     "iopub.status.busy": "2026-01-03T17:48:21.763277Z",
     "iopub.status.idle": "2026-01-03T17:48:29.017791Z",
     "shell.execute_reply": "2026-01-03T17:48:29.017171Z",
     "shell.execute_reply.started": "2026-01-03T17:48:21.764248Z"
    },
    "id": "GLNNqwV4ysNP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create your wandb run\n",
    "run = wandb.init(\n",
    "    name = f\"<YOUR-RUN-NAME-HERE>\", ### Change this to your desired run name\n",
    "    # reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
    "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"<ENTER-YOUR-PROJECT-NAME-HERE>\", ### Project should be created in your wandb account\n",
    "    config = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0RrtpFKzH3k"
   },
   "source": [
    "# Checkpointing and Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:48:33.440974Z",
     "iopub.status.busy": "2026-01-03T17:48:33.440625Z",
     "iopub.status.idle": "2026-01-03T17:48:33.446932Z",
     "shell.execute_reply": "2026-01-03T17:48:33.446169Z",
     "shell.execute_reply.started": "2026-01-03T17:48:33.440942Z"
    },
    "id": "O1gbAkMtlWHk",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ensure checkpoint directory exists\n",
    "checkpoint_dir = config.get(\"checkpoint_dir\", \"./checkpoints\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, scheduler, metrics, epoch, path):\n",
    "    \"\"\"\n",
    "    Saves model, optimizer, scheduler, and training metrics to a checkpoint.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to save\n",
    "        optimizer (Optimizer): Optimizer\n",
    "        scheduler (LRScheduler): Learning rate scheduler\n",
    "        metrics (dict): Dictionary of tracked metrics\n",
    "        epoch (int): Current epoch\n",
    "        path (str): Path to save checkpoint\n",
    "    \"\"\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict() if optimizer is not None else None,\n",
    "            \"scheduler_state_dict\": scheduler.state_dict() if scheduler is not None else None,\n",
    "            \"metrics\": metrics,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "        path,\n",
    "    )\n",
    "    print(f\"Checkpoint saved at {path}\")\n",
    "\n",
    "\n",
    "def load_model(model, optimizer=None, scheduler=None, path=\"./checkpoint.pth\", device=None):\n",
    "    \"\"\"\n",
    "    Loads model, optimizer, scheduler, and metrics from a checkpoint.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to load weights into\n",
    "        optimizer (Optimizer, optional): Optimizer to load state\n",
    "        scheduler (LRScheduler, optional): Scheduler to load state\n",
    "        path (str): Path to checkpoint file\n",
    "        device (torch.device, optional): Device mapping for checkpoint\n",
    "\n",
    "    Returns:\n",
    "        tuple: model, optimizer, scheduler, epoch, metrics\n",
    "    \"\"\"\n",
    "    map_location = device if device is not None else \"cpu\"\n",
    "    checkpoint = torch.load(path, map_location=map_location)\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "    if optimizer is not None and checkpoint.get(\"optimizer_state_dict\") is not None:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    if scheduler is not None and checkpoint.get(\"scheduler_state_dict\") is not None:\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "    epoch = checkpoint.get(\"epoch\", 0)\n",
    "    metrics = checkpoint.get(\"metrics\", {})\n",
    "\n",
    "    print(f\"Checkpoint loaded from {path} (epoch {epoch})\")\n",
    "    return model, optimizer, scheduler, epoch, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpFT7iriy5bi"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T17:48:47.246058Z",
     "iopub.status.busy": "2026-01-03T17:48:47.245737Z"
    },
    "id": "59FcCeJfy3Zm",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training setup\n",
    "start_epoch = 0\n",
    "best_cls_acc = 0.0\n",
    "best_ret_acc = 0.0\n",
    "eval_cls = True\n",
    "\n",
    "for epoch in range(start_epoch, config[\"epochs\"]):\n",
    "    print(f\"\\n=== Epoch {epoch + 1}/{config['epochs']} ===\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Train\n",
    "    # -----------------------------\n",
    "    train_cls_acc, train_loss = train_epoch(\n",
    "        model, cls_train_loader, optimizer, scheduler, scaler, DEVICE, criterion\n",
    "    )\n",
    "    curr_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(\n",
    "        f\"Train | Cls Acc: {train_cls_acc:.4f}% | Loss: {train_loss:.4f} | LR: {curr_lr:.6f}\"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"train_cls_acc\": train_cls_acc,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"lr\": curr_lr,\n",
    "    }\n",
    "\n",
    "    # -----------------------------\n",
    "    # Classification Validation\n",
    "    # -----------------------------\n",
    "    if eval_cls:\n",
    "        valid_cls_acc, valid_loss = valid_epoch_cls(model, cls_val_loader, DEVICE, criterion)\n",
    "        print(f\"Val (Cls) | Acc: {valid_cls_acc:.4f}% | Loss: {valid_loss:.4f}\")\n",
    "        metrics.update({\n",
    "            \"valid_cls_acc\": valid_cls_acc,\n",
    "            \"valid_loss\": valid_loss,\n",
    "        })\n",
    "\n",
    "    # -----------------------------\n",
    "    # Verification / Retrieval Validation\n",
    "    # -----------------------------\n",
    "    valid_ret_metrics = valid_epoch_ver(model, ver_val_loader, DEVICE)\n",
    "    valid_ret_acc = valid_ret_metrics[\"ACC\"]\n",
    "    print(f\"Val (Veri) | ACC: {valid_ret_acc:.4f}%\")\n",
    "    metrics.update({\"valid_ret_acc\": valid_ret_acc})\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save checkpoints\n",
    "    # -----------------------------\n",
    "    checkpoint_path = os.path.join(config[\"checkpoint_dir\"], \"last.pth\")\n",
    "    save_model(model, optimizer, scheduler, metrics, epoch, checkpoint_path)\n",
    "    print(f\"Saved last epoch model: {checkpoint_path}\")\n",
    "\n",
    "    # Save best classification model\n",
    "    if eval_cls and valid_cls_acc >= best_cls_acc:\n",
    "        best_cls_acc = valid_cls_acc\n",
    "        best_cls_path = os.path.join(config[\"checkpoint_dir\"], \"best_cls.pth\")\n",
    "        save_model(model, optimizer, scheduler, metrics, epoch, best_cls_path)\n",
    "        if \"wandb\" in globals():\n",
    "            wandb.save(best_cls_path)\n",
    "        print(f\"Saved best classification model: {best_cls_path}\")\n",
    "\n",
    "    # Save best retrieval model\n",
    "    if valid_ret_acc >= best_ret_acc:\n",
    "        best_ret_acc = valid_ret_acc\n",
    "        best_ret_path = os.path.join(config[\"checkpoint_dir\"], \"best_ret.pth\")\n",
    "        save_model(model, optimizer, scheduler, metrics, epoch, best_ret_path)\n",
    "        if \"wandb\" in globals():\n",
    "            wandb.save(best_ret_path)\n",
    "        print(f\"Saved best retrieval model: {best_ret_path}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Log metrics\n",
    "    # -----------------------------\n",
    "    if \"run\" in globals() and run is not None:\n",
    "        run.log(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXLTfQjv0cCb"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:50:56.088870Z",
     "iopub.status.busy": "2026-01-03T14:50:56.088307Z",
     "iopub.status.idle": "2026-01-03T14:50:56.096862Z",
     "shell.execute_reply": "2026-01-03T14:50:56.096290Z",
     "shell.execute_reply.started": "2026-01-03T14:50:56.088839Z"
    },
    "id": "XUAa3m2h0eCD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_epoch_ver(model, pair_dataloader, device):\n",
    "    \"\"\"\n",
    "    Runs inference on verification pairs and returns similarity scores.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model\n",
    "        pair_dataloader (DataLoader): DataLoader yielding (img1, img2) tuples\n",
    "        device (torch.device): Device for inference\n",
    "\n",
    "    Returns:\n",
    "        list: Similarity scores (float) for each pair\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    scores = []\n",
    "\n",
    "    # TODO: Implement the test epoch for verification\n",
    "    # Hint: Follow the structure of valid_epoch_ver but without labels\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiKm3eHgs5hY"
   },
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T14:51:25.083868Z",
     "iopub.status.busy": "2026-01-03T14:51:25.083190Z",
     "iopub.status.idle": "2026-01-03T14:51:25.947858Z",
     "shell.execute_reply": "2026-01-03T14:51:25.947135Z",
     "shell.execute_reply.started": "2026-01-03T14:51:25.083841Z"
    },
    "id": "Gj8RkRXtAx3-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scores = test_epoch_ver(model, ver_test_loader, DEVICE)\n",
    "\n",
    "# -----------------------------\n",
    "# Finish wandb run (optional)\n",
    "# -----------------------------\n",
    "if \"run\" in globals() and run is not None:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNNbgtDIrytI"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgUZa2KuR6qN"
   },
   "source": [
    "## Kaggle Submission Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umgIR_1mSUkA"
   },
   "source": [
    "* Run the code block below to **automatically generate** the submission file for the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T12:25:36.465968Z",
     "iopub.status.busy": "2025-12-30T12:25:36.465614Z",
     "iopub.status.idle": "2025-12-30T12:25:57.127298Z",
     "shell.execute_reply": "2025-12-30T12:25:57.126513Z",
     "shell.execute_reply.started": "2025-12-30T12:25:36.465939Z"
    },
    "id": "C7Mu5I001pWW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame({\n",
    "    \"ID\": range(len(scores)),\n",
    "    \"Label\": scores,\n",
    "})\n",
    "print(df_submission.head())\n",
    "submission_path = \"verification_early_submission.csv\"\n",
    "df_submission.to_csv(submission_path, index=False)\n",
    "print(f\"Saved submission to {submission_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLDOjjhKSNqW"
   },
   "source": [
    "* Run the code block below to **automatically submit** the generated submission file to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Av0oAUtd1pWX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "api.competition_submit(file_name=\"verification_early_submission.csv\", message=\"<ENTER_YOUR_MESSAGE_HERE>\", competition=\"11785-hw-2-p-2-face-verification-spring-2026\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJhOCAQtSjJZ"
   },
   "source": [
    "\n",
    "#### **Manual submission (optional)**\n",
    "\n",
    "If automatic submission is unavailable, download the CSV and submit it manually using one of the options below.\n",
    "\n",
    "##### **Kaggle**\n",
    "\n",
    "* After running the cell above, the file **`verification_submission.csv`** will be saved in:\n",
    "  **Right sidebar ‚Üí Output ‚Üí `kaggle/working/`**\n",
    "* If the file does not appear right away, click the **refresh icon** in the top-right corner of the Output panel.\n",
    "* Once the file is visible, **right-click ‚Üí Download**.\n",
    "* Upload the downloaded file to the **Kaggle competition submission page**.\n",
    "\n",
    "---\n",
    "\n",
    "##### **Colab**\n",
    "\n",
    "```python\n",
    "from google.colab import files\n",
    "files.download(\"verification_submission.csv\")\n",
    "```\n",
    "\n",
    "##### **PSC**\n",
    "\n",
    "* Open the left file browser.\n",
    "* Navigate to:\n",
    "  `/jet/home/<your_username>/`\n",
    "* Locate **`verification_submission.csv`**.\n",
    "* Right-click the file and select **Download**.\n",
    "* If the file does not appear immediately, refresh the file browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVIpL5xhSryR"
   },
   "source": [
    "## üì¶ Autolab Submission Instructions (Read Carefully)\n",
    "\n",
    "This section guides you through creating and submitting your **final Autolab code submission**.\n",
    "Please follow each step in order. Skipping steps may result in an incomplete or invalid submission.\n",
    "\n",
    "### ‚è∞ Important Deadlines\n",
    "\n",
    "* **Kaggle Final Deadline:** See the course schedule\n",
    "* **Autolab Code Submission Deadline:**\n",
    "  **48 hours after** the Kaggle deadline (or the same day as your Slack Kaggle submission, if applicable)\n",
    "\n",
    "You must submit **both**:\n",
    "\n",
    "1. A valid Kaggle submission\n",
    "2. A complete Autolab code submission zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YM36_DEZJcM"
   },
   "source": [
    "### Step 1: Generate Your Model Metadata File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vinxbiuVZJcO"
   },
   "source": [
    "Please assign your **final trained model** to the global variable `MODEL`.\n",
    "\n",
    "We will use this variable to automatically generate a file named\n",
    "`model_metadata.json`, which records important information about your model (such as parameter count and architecture).\n",
    "\n",
    "#### üî¥ Important Requirements\n",
    "\n",
    "* **`MODEL` must be the exact model used for your best Kaggle submission**\n",
    "* This step is **required** for grading and audit purposes\n",
    "* If the model does not match your Kaggle submission, your score may be invalidated\n",
    "\n",
    "#### üïí When to Do This\n",
    "\n",
    "* Run this cell **only after you have finished training** your final model\n",
    "* Do **not** retrain or modify the model after assigning it to `MODEL`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tf2_M-bmalm-"
   },
   "outputs": [],
   "source": [
    "MODEL = model # TODO: Initialize to your tained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-69pTbKZJcQ"
   },
   "source": [
    "### Step 2: Complete the README Section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhJ8eaH7ZJcQ"
   },
   "source": [
    "Fill in the `README` variable with a brief description of your work:\n",
    "\n",
    "* **Model**: Architecture and key design choices\n",
    "* **Training Strategy**: Optimizer, scheduler, loss, etc.\n",
    "* **Augmentations**: If used (omit if not)\n",
    "* **Notebook Execution**: Any instructions needed to run your notebook\n",
    "\n",
    "This README will be included in your submission zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCeAyINIZJcQ"
   },
   "outputs": [],
   "source": [
    "README = \"\"\"\n",
    "- **Model**: Model archtiecture description. Anything unique? Any specific architecture shapes or strategies?\n",
    "- **Training Strategy**: optimizer + scheduler + loss function + any other unique ideas\n",
    "- **Augmentations**: augmentations if used. If augmentations weren't used, then ignore\n",
    "- **Notebook Execution**: Any instructions required to run your notebook.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RH8MuX6oZJcR"
   },
   "source": [
    "### Step 3: Provide Required Credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ksSKSe3ZJcR"
   },
   "source": [
    "Ensure the following variables are correctly set:\n",
    "\n",
    "* `KAGGLE_USERNAME`\n",
    "* `KAGGLE_API_KEY`\n",
    "* `WANDB_API_KEY`\n",
    "* `WANDB_USERNAME_OR_TEAMNAME`\n",
    "* `WANDB_PROJECT`\n",
    "\n",
    "These are used **only** to:\n",
    "\n",
    "* Fetch your official Kaggle score\n",
    "* Export your top WandB runs for grading\n",
    "\n",
    "‚ö†Ô∏è Do **not** share your API keys publicly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87Sm8LDkZJcR"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "KAGGLE_USERNAME = \"<your-username>\" # TODO\n",
    "KAGGLE_API_KEY  = \"<your-key>\"      # TODO\n",
    "WANDB_API_KEY   = \"<ENTER-YOUR-WANDB-API-KEY-HERE>\" # TODO\n",
    "WANDB_USERNAME_OR_TEAMNAME = \"<ENTER-YOUR-WANDB-USERNAME-OR-TEAMNAME-HERE>\" # TODO: Put your username-or-team-name here\n",
    "WANDB_PROJECT              = \"<ENTER-YOUR-WANDB-PROJECT-NAME-HERE>\" # TODO: Put your project-name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyUlWU8JZJcR"
   },
   "source": [
    "### Step 4: Set File Paths Correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH3WBLcFZJcR"
   },
   "source": [
    "You must provide absolute path to your **final notebook**\n",
    "\n",
    "### Platform-specific guidance\n",
    "\n",
    "**Colab**\n",
    "\n",
    "* Right-click a file in the left file pane ‚Üí **Copy path**\n",
    "* Paths typically start with `/content/...`\n",
    "\n",
    "**Kaggle**\n",
    "\n",
    "* Download your notebook (`File ‚Üí Download Notebook`)\n",
    "* Upload it via **Upload Input ‚Üí Upload Model**\n",
    "* Copy paths from the right sidebar (`/kaggle/working/...`)\n",
    "\n",
    "**PSC / Linux**\n",
    "\n",
    "* Locate files under `/jet/home/<your_username>/`\n",
    "* Use `!ls` to confirm paths\n",
    "\n",
    "Paste the correct path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqwRboVuZJcS"
   },
   "outputs": [],
   "source": [
    "NOTEBOOK_PATH = \"<ENTER-YOUR-NOTEBOOK-PATH-HERE>\" # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuAc7uEvZJcS"
   },
   "source": [
    "### Step 5: (Optional) Additional Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqNhYlKLZJcS"
   },
   "source": [
    "If you have extra files you want to include (e.g., config files, scripts), add their paths to `ADDITIONAL_FILES`, Otherwise, leave this list empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7KhIHXTZJcS"
   },
   "outputs": [],
   "source": [
    "ADDITIONAL_FILES = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RIw_hgNZJcU"
   },
   "source": [
    "### Step 6: Generate the Final Submission Zip\n",
    "\n",
    "Before running the submission cell:\n",
    "\n",
    "* Ensure `ACKNOWLEDGED = True`\n",
    "* Double-check all paths and credentials\n",
    "* Confirm your Kaggle submission exists\n",
    "\n",
    "Then run the cell below.\n",
    "This should generate a `HW2P2_final_submission.zip` which includes:\n",
    "* Your notebook\n",
    "* `model_metadata_*.json`\n",
    "* README\n",
    "* WandB run exports\n",
    "* Kaggle metadata\n",
    "* Academic integrity acknowledgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVqcC7H3SzDQ"
   },
   "outputs": [],
   "source": [
    "#### DO NOT MODIFY ####\n",
    "!git clone https://github.com/CMU-IDeeL/S26-HWP2-Submission-Backend.git\n",
    "!mv S26-HWP2-Submission-Backend/submission .\n",
    "!rm -rf S26-HWP2-Submission-Backend\n",
    "from submission.submission_config import SubmissionConfig\n",
    "from submission.backend_config import BackendConfig, HW2P2_BACKEND_CONFIG\n",
    "from submission.main import create_submission_zip\n",
    "\n",
    "create_submission_zip(\n",
    "    cfg = SubmissionConfig(\n",
    "        model = MODEL,\n",
    "        kaggle_username  = KAGGLE_USERNAME,\n",
    "        kaggle_api_key   = KAGGLE_API_KEY,\n",
    "        wandb_api_key    = WANDB_API_KEY,\n",
    "        wandb_entity     = WANDB_USERNAME_OR_TEAMNAME,\n",
    "        wandb_project    = WANDB_PROJECT,\n",
    "        acknowledged     = ACKNOWLEDGED,\n",
    "        readme           = README,\n",
    "        notebook_path    = NOTEBOOK_PATH,\n",
    "        additional_files = ADDITIONAL_FILES\n",
    "    ),\n",
    "    backend_cfg = HW2P2_BACKEND_CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-RnvojFZJcU"
   },
   "source": [
    "### Step 7: Upload to Autolab\n",
    "\n",
    "1. Download `HW2P2_final_submission.zip`\n",
    "2. Upload it to **Autolab**\n",
    "3. Verify the upload succeeds\n",
    "\n",
    "‚ö†Ô∏è Submitting an incomplete zip or modifying backend code may result in grading penalties or an Academic Integrity Violation.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Final Checklist\n",
    "\n",
    "Before submitting, confirm:\n",
    "\n",
    "* [ ] Kaggle username is valid\n",
    "* [ ] `model_metadata_*.json` matches your best run\n",
    "* [ ] README is complete\n",
    "* [ ] File paths are correct\n",
    "* [ ] `ACKNOWLEDGED = True`\n",
    "* [ ] Final zip downloads successfully\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "b9EpnOYJ3kG1",
    "p5VKNQO43kG2",
    "E8G-S6Qu3kG6",
    "L1yqsDZK3kG7",
    "aD8oEOiY3kG8",
    "M3VWeHyS3kG9",
    "TrdNB7VI3kHD",
    "Q1XE_p9OsQGp",
    "9OgkfYwP7HVt",
    "L4QjjC6xnMFs",
    "EEAW65sB8Wlp",
    "ZzxNNzGe8ccB",
    "mPSk8DyK8htk",
    "436KzM6u-3A2",
    "niSJ49lzpHgW",
    "KZPVGXOu59Wh",
    "y3TUocDw_JU_",
    "6L9AffxhubP8",
    "-d5ZDQfpw7gR",
    "juUbZnP0AEUi",
    "0q1gRMAsyknz",
    "piblCbe5yotj",
    "t0RrtpFKzH3k",
    "wpFT7iriy5bi",
    "VXLTfQjv0cCb",
    "FgUZa2KuR6qN",
    "zVIpL5xhSryR"
   ],
   "gpuType": "A100",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
